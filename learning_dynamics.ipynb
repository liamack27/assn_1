{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "name": "learning_dynamics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kiuxBR04DcZ"
      },
      "source": [
        "# Dynamics Regression and Graphical Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpqbPGN731Z8"
      },
      "source": [
        "## Notebook Setup \n",
        "The following cell will checkout the underactuated repository, and set up the path (only if necessary).\n",
        "- On Google's Colaboratory, this **will take approximately two minutes** on the first time it runs (to provision the machine), but should only need to reinstall once every 12 hours.  Colab will ask you to \"Reset all runtimes\"; say no to save yourself the reinstall.\n",
        "- On Binder, the machines should already be provisioned by the time you can run this; it should return (almost) instantly.\n",
        "\n",
        "More details are available [here](http://underactuated.mit.edu/drake.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvu5-Bm531Z-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b02ec7d-4696-48f9-bf35-6c8bcb6829bb"
      },
      "source": [
        "import importlib\n",
        "import os, sys\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "# Install drake (and underactuated).\n",
        "if 'google.colab' in sys.modules and importlib.util.find_spec('underactuated') is None:\n",
        "    urlretrieve(f\"http://underactuated.csail.mit.edu/scripts/setup/setup_underactuated_colab.py\",\n",
        "                \"setup_underactuated_colab.py\")\n",
        "    from setup_underactuated_colab import setup_underactuated\n",
        "    setup_underactuated(underactuated_sha='1dca1b915977aeec9f327d61aaeac4cfb9c6b408', drake_version='0.25.0', drake_build='releases')\n",
        "\n",
        "server_args = []\n",
        "if 'google.colab' in sys.modules:\n",
        "  server_args = ['--ngrok_http_tunnel']\n",
        "\n",
        "# Setup matplotlib.  \n",
        "from IPython import get_ipython\n",
        "if get_ipython() is not None: get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
        "\n",
        "# python libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# underactuated imports\n",
        "from underactuated import plot_2d_phase_portrait"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/opt/underactuated'...\n",
            "\n",
            "HEAD is now at 1dca1b9 test hotfix for python3.7 on colab\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fjIsN4431aA"
      },
      "source": [
        "## Problem Description\n",
        "In this problem you will implement a simple neural network in pytorch to model the dynamics of the damped pendulum seen in class from data. At the end of the notebook, you will perform a graphical analysis and answer a few questions about the system.\n",
        "\n",
        "**These are the main steps of the exercise:**\n",
        "1. Implement the network architecture in pytorch\n",
        "2. Perform a graphical analysis of the dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNO98ajN_z1"
      },
      "source": [
        "## Sampling measurements\n",
        "In class, we derived the dynamics for the damped pendulum system analytically:\n",
        "\n",
        "> $b\\dot\\theta = u_0 - mgl\\sin\\theta$\n",
        "\n",
        "In this problem, we pretend we don't know the relationship $\\dot \\theta = f(\\theta)$ and would like to learn it from $(\\theta, \\dot \\theta)$ data measurements. That is, we'd like to train a neural network $NN(\\theta) \\approx f(\\theta) = \\dot \\theta$.\n",
        "\n",
        "In the cell below, we assume a pendulum with $l = 1$ and $m = 1$ for simplicity. Gravity is $g = 9.81$, and we assume 0 torque. We generate 10000 evenly spaced samples from the ground truth equations, and then sample 100 examples and add some random noise to \"simulate\" taking 100 measurements. The data we're trying to fit is plotted below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akkyz7AzRTta"
      },
      "source": [
        "### Pytorch tip\n",
        "\n",
        "One tip you'll find useful when debugging is to fix the random seed generators for your machine learning library (in our case pytorch), and potentially for numpy as well. This allows us to ensure that our neural network will be initialized with the same values each time we run our code, and it will allow us to see the effects of our changes and make things easier to debug. We can fix the random generator seed as seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuUod6sRZG68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "ece867c1-3594-414d-dfa5-805d00957aed"
      },
      "source": [
        "np.random.seed(5) # you can pick any integer for different seeds\n",
        "torch.manual_seed(5)\n",
        "\n",
        "# System parameters. Feel free to play with these, but use\n",
        "# the original set for the autograder!\n",
        "m = 1\n",
        "l = 1\n",
        "g = 9.81\n",
        "u = 0\n",
        "\n",
        "def generate_measurements():\n",
        "  # create ground truth data points\n",
        "  theta = torch.unsqueeze(torch.linspace(-2*np.pi, 2*np.pi, 10000), dim=1)\n",
        "\n",
        "  # subsample 100 ground truth data points and add noise\n",
        "  idxs = np.random.choice(10000, 100)\n",
        "  theta = theta[idxs]\n",
        "  theta_dot = u - m*g*l*torch.sin(theta) + 1.5*torch.rand(theta.size()) - 1.5*torch.rand(theta.size())\n",
        "\n",
        "  return theta, theta_dot\n",
        "\n",
        "# plot sampled measurements\n",
        "theta, theta_dot = generate_measurements()\n",
        "plt.scatter(theta, theta_dot, s=5)\n",
        "plt.xlabel('theta')\n",
        "plt.ylabel('theta_dot')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'theta_dot')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N_XNXq0QveP"
      },
      "source": [
        "## Defining our Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz3sl9MCSCAd"
      },
      "source": [
        "We will now implement our neural network in pytorch. Pytorch allows us to define a model class as a subclass of `nn.Module` (checkout the [pytorch docs](https://pytorch.org/docs/stable/index.html) for more information). We define our network architecture at initialization, and then we define a `forward` method that defines our forward pass. This method takes in the inputs to our neural network, and returns the outputs. \n",
        "\n",
        "We've provided an example model that shows you how to stack two linear layers with a Leaky ReLU nonlinearity in between them. Your job is to define `self.model`.\n",
        "\n",
        "- Layer 1: linear layer with 1 input and 200 outputs\n",
        "- Leaky ReLU nonlinearity\n",
        "- Layer 2: linear layer with 200 inputs and 100 outputs\n",
        "- Leaky ReLU nonlinearity\n",
        "- Layer 3: linear layer with 100 inputs and 1 output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4didPA2EDhN"
      },
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "\n",
        "    # Example model\n",
        "    self.example_model = nn.Sequential(\n",
        "        torch.nn.Linear(1, 50),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(50, 1)\n",
        "    )\n",
        "    \n",
        "    ####################################\n",
        "    # Define your model\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        torch.nn.Linear(1, 200),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(200, 100),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(100, 1)\n",
        "        # YOUR CODE HERE\n",
        "    )\n",
        "    \n",
        "    ####################################\n",
        "\n",
        "  def forward(self, theta):\n",
        "    theta_dot_hat = self.model(theta)\n",
        "    return theta_dot_hat"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5soWOWETglM"
      },
      "source": [
        "## Training our Network\n",
        "\n",
        "Now we can instantiate a network, an optimizing algorithm that will update the weights of our network, and a loss function. We will be using a mean-squared error loss for this problem. We are going to use the following optimizer and loss function:\n",
        "\n",
        "- Optimizer: [Adam optimizer](https://pytorch.org/docs/stable/optim.html)\n",
        "- Loss function: [Mean-squared Error loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)\n",
        "\n",
        "In the cell below, your job is to instantiate a mean-squared error loss, and an Adam optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMfELFEPVImq"
      },
      "source": [
        "We then iteratively update the parameters of our network. This is done by feeding in the theta values we measured, and computing a loss using the outputs (the networks \"guess\" for the theta dot outputs). If our predicted theta dots are far from the theta dots we measured, then our loss will be high and vice versa. We use the loss to call `loss.backward()`, pytorch runs [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) to update the network weights we passed into the Adam optimizer at initialization for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyiw_7dVyeUZ"
      },
      "source": [
        "### Pytorch tip\n",
        "\n",
        "Note that we zero out our gradients before we step through our optimizer at each iteration. We need to do this because pytorch accumulates gradients after each pass. This is used to train other types of networks like RNN's, but for our purposes here we want just the new gradients at each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EufqWTI0FjYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe2ed97-8562-4247-c7b6-6c2cfc86129c"
      },
      "source": [
        "np.random.seed(5) # you can pick any integer for different seeds\n",
        "torch.manual_seed(5)\n",
        "\n",
        "# Feel free to play with these, but keep the original values for the autograder.\n",
        "iterations = 300 # number of optimization iterations\n",
        "learning_rate = 0.05 # you do not need to tune this!\n",
        "\n",
        "# Generate measurements\n",
        "theta, theta_dot = generate_measurements()\n",
        "\n",
        "# Initialize our network, optimizer, and loss function\n",
        "NN = Network()\n",
        "\n",
        "####################################\n",
        "# Instantiate a mean-squared error loss\n",
        "# and an Adam optimizer. You can access\n",
        "# your networks parameters with NN.parameters().\n",
        "# You will need to pass this in as an\n",
        "# argument when instantiating the optimizer.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "optimizer = torch.optim.Adam(NN.parameters(), lr=learning_rate)\n",
        "L = torch.nn.MSELoss()\n",
        "\n",
        "####################################\n",
        "\n",
        "final_loss = 0.0\n",
        "if optimizer is not None and L is not None:\n",
        "  for t in range(iterations):\n",
        "\n",
        "    theta_dot_hat = NN(theta) # pass data through your neural net, and generate x_ddot predictions\n",
        "\n",
        "    loss = L(theta_dot_hat, theta_dot) # compare your predictions with the measured x_ddot values from your data set\n",
        "\n",
        "    print('Loss @ time {}: {}'.format(t, loss.item()))\n",
        "\n",
        "    optimizer.zero_grad() # clear gradients from the last iteration, before generating gradients for this iteration\n",
        "    loss.backward() # run backpropagation, and compute gradients\n",
        "    optimizer.step() # apply gradients to your network weights\n",
        "\n",
        "  final_loss = loss.item()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss @ time 0: 48.07992172241211\n",
            "Loss @ time 1: 207.51513671875\n",
            "Loss @ time 2: 231.2499237060547\n",
            "Loss @ time 3: 129.2787322998047\n",
            "Loss @ time 4: 129.3562469482422\n",
            "Loss @ time 5: 81.83065795898438\n",
            "Loss @ time 6: 49.85768508911133\n",
            "Loss @ time 7: 43.55738830566406\n",
            "Loss @ time 8: 45.533470153808594\n",
            "Loss @ time 9: 47.52522277832031\n",
            "Loss @ time 10: 48.8486328125\n",
            "Loss @ time 11: 49.33253860473633\n",
            "Loss @ time 12: 48.2778434753418\n",
            "Loss @ time 13: 47.41399002075195\n",
            "Loss @ time 14: 46.687618255615234\n",
            "Loss @ time 15: 44.33909606933594\n",
            "Loss @ time 16: 42.725059509277344\n",
            "Loss @ time 17: 41.46963882446289\n",
            "Loss @ time 18: 40.27759552001953\n",
            "Loss @ time 19: 39.193721771240234\n",
            "Loss @ time 20: 38.262474060058594\n",
            "Loss @ time 21: 37.450923919677734\n",
            "Loss @ time 22: 36.6458625793457\n",
            "Loss @ time 23: 35.617977142333984\n",
            "Loss @ time 24: 34.304466247558594\n",
            "Loss @ time 25: 33.368167877197266\n",
            "Loss @ time 26: 32.8609504699707\n",
            "Loss @ time 27: 32.230567932128906\n",
            "Loss @ time 28: 30.979816436767578\n",
            "Loss @ time 29: 30.06073760986328\n",
            "Loss @ time 30: 29.48105239868164\n",
            "Loss @ time 31: 28.50572967529297\n",
            "Loss @ time 32: 28.277246475219727\n",
            "Loss @ time 33: 27.081331253051758\n",
            "Loss @ time 34: 26.793508529663086\n",
            "Loss @ time 35: 25.998790740966797\n",
            "Loss @ time 36: 25.814170837402344\n",
            "Loss @ time 37: 25.218320846557617\n",
            "Loss @ time 38: 24.633451461791992\n",
            "Loss @ time 39: 24.496347427368164\n",
            "Loss @ time 40: 23.737634658813477\n",
            "Loss @ time 41: 23.28948211669922\n",
            "Loss @ time 42: 22.741270065307617\n",
            "Loss @ time 43: 21.912126541137695\n",
            "Loss @ time 44: 21.566312789916992\n",
            "Loss @ time 45: 20.856874465942383\n",
            "Loss @ time 46: 19.871490478515625\n",
            "Loss @ time 47: 19.269624710083008\n",
            "Loss @ time 48: 18.591537475585938\n",
            "Loss @ time 49: 17.504789352416992\n",
            "Loss @ time 50: 16.534467697143555\n",
            "Loss @ time 51: 15.838187217712402\n",
            "Loss @ time 52: 15.148265838623047\n",
            "Loss @ time 53: 14.247882843017578\n",
            "Loss @ time 54: 12.50963020324707\n",
            "Loss @ time 55: 10.455981254577637\n",
            "Loss @ time 56: 8.323159217834473\n",
            "Loss @ time 57: 7.899564743041992\n",
            "Loss @ time 58: 6.996045112609863\n",
            "Loss @ time 59: 5.706342697143555\n",
            "Loss @ time 60: 5.410121917724609\n",
            "Loss @ time 61: 4.906597137451172\n",
            "Loss @ time 62: 4.6415019035339355\n",
            "Loss @ time 63: 3.976376533508301\n",
            "Loss @ time 64: 4.100494861602783\n",
            "Loss @ time 65: 4.370097637176514\n",
            "Loss @ time 66: 3.95289945602417\n",
            "Loss @ time 67: 3.9730639457702637\n",
            "Loss @ time 68: 3.7215216159820557\n",
            "Loss @ time 69: 3.301166296005249\n",
            "Loss @ time 70: 3.5564026832580566\n",
            "Loss @ time 71: 3.502455234527588\n",
            "Loss @ time 72: 3.162536382675171\n",
            "Loss @ time 73: 3.286728620529175\n",
            "Loss @ time 74: 3.115156888961792\n",
            "Loss @ time 75: 2.8483147621154785\n",
            "Loss @ time 76: 2.982907772064209\n",
            "Loss @ time 77: 2.8991928100585938\n",
            "Loss @ time 78: 2.7936577796936035\n",
            "Loss @ time 79: 2.7098147869110107\n",
            "Loss @ time 80: 2.680602788925171\n",
            "Loss @ time 81: 2.6518290042877197\n",
            "Loss @ time 82: 2.435365915298462\n",
            "Loss @ time 83: 2.468884229660034\n",
            "Loss @ time 84: 2.3536300659179688\n",
            "Loss @ time 85: 2.2005960941314697\n",
            "Loss @ time 86: 2.206641674041748\n",
            "Loss @ time 87: 2.1253507137298584\n",
            "Loss @ time 88: 2.035125494003296\n",
            "Loss @ time 89: 1.9497840404510498\n",
            "Loss @ time 90: 1.8871638774871826\n",
            "Loss @ time 91: 1.769313931465149\n",
            "Loss @ time 92: 1.6789004802703857\n",
            "Loss @ time 93: 1.8934636116027832\n",
            "Loss @ time 94: 1.9773029088974\n",
            "Loss @ time 95: 1.5245765447616577\n",
            "Loss @ time 96: 1.3589754104614258\n",
            "Loss @ time 97: 1.5283046960830688\n",
            "Loss @ time 98: 1.3611842393875122\n",
            "Loss @ time 99: 1.1140257120132446\n",
            "Loss @ time 100: 1.1561819314956665\n",
            "Loss @ time 101: 1.1087325811386108\n",
            "Loss @ time 102: 0.8908539414405823\n",
            "Loss @ time 103: 0.8619107007980347\n",
            "Loss @ time 104: 1.0296869277954102\n",
            "Loss @ time 105: 1.0972585678100586\n",
            "Loss @ time 106: 1.1695773601531982\n",
            "Loss @ time 107: 1.0137476921081543\n",
            "Loss @ time 108: 0.748603343963623\n",
            "Loss @ time 109: 0.634906530380249\n",
            "Loss @ time 110: 0.7914485931396484\n",
            "Loss @ time 111: 0.9118276834487915\n",
            "Loss @ time 112: 0.6753907799720764\n",
            "Loss @ time 113: 0.47616350650787354\n",
            "Loss @ time 114: 0.6073057651519775\n",
            "Loss @ time 115: 0.7622656226158142\n",
            "Loss @ time 116: 0.6217954158782959\n",
            "Loss @ time 117: 0.4268793761730194\n",
            "Loss @ time 118: 0.48835545778274536\n",
            "Loss @ time 119: 0.6239680647850037\n",
            "Loss @ time 120: 0.5674140453338623\n",
            "Loss @ time 121: 0.4381083548069\n",
            "Loss @ time 122: 0.3962900936603546\n",
            "Loss @ time 123: 0.44542816281318665\n",
            "Loss @ time 124: 0.474272757768631\n",
            "Loss @ time 125: 0.45026230812072754\n",
            "Loss @ time 126: 0.4015516936779022\n",
            "Loss @ time 127: 0.3659248650074005\n",
            "Loss @ time 128: 0.371724396944046\n",
            "Loss @ time 129: 0.40916168689727783\n",
            "Loss @ time 130: 0.416513592004776\n",
            "Loss @ time 131: 0.3695668876171112\n",
            "Loss @ time 132: 0.3270534574985504\n",
            "Loss @ time 133: 0.3336675763130188\n",
            "Loss @ time 134: 0.3715659976005554\n",
            "Loss @ time 135: 0.38315096497535706\n",
            "Loss @ time 136: 0.3577611446380615\n",
            "Loss @ time 137: 0.3242567777633667\n",
            "Loss @ time 138: 0.3113018274307251\n",
            "Loss @ time 139: 0.3160054385662079\n",
            "Loss @ time 140: 0.3265567719936371\n",
            "Loss @ time 141: 0.3315204679965973\n",
            "Loss @ time 142: 0.3284114897251129\n",
            "Loss @ time 143: 0.3245287835597992\n",
            "Loss @ time 144: 0.31851670145988464\n",
            "Loss @ time 145: 0.3116823434829712\n",
            "Loss @ time 146: 0.30273833870887756\n",
            "Loss @ time 147: 0.2950281798839569\n",
            "Loss @ time 148: 0.2924900949001312\n",
            "Loss @ time 149: 0.2929607331752777\n",
            "Loss @ time 150: 0.2980494201183319\n",
            "Loss @ time 151: 0.3044225871562958\n",
            "Loss @ time 152: 0.3118465542793274\n",
            "Loss @ time 153: 0.3238529860973358\n",
            "Loss @ time 154: 0.34673815965652466\n",
            "Loss @ time 155: 0.3972453773021698\n",
            "Loss @ time 156: 0.4807303547859192\n",
            "Loss @ time 157: 0.6151645183563232\n",
            "Loss @ time 158: 0.7202892899513245\n",
            "Loss @ time 159: 0.5991796255111694\n",
            "Loss @ time 160: 0.5123622417449951\n",
            "Loss @ time 161: 0.5399084091186523\n",
            "Loss @ time 162: 0.577233612537384\n",
            "Loss @ time 163: 0.5083101391792297\n",
            "Loss @ time 164: 0.38011565804481506\n",
            "Loss @ time 165: 0.292464017868042\n",
            "Loss @ time 166: 0.3469540774822235\n",
            "Loss @ time 167: 0.4566880762577057\n",
            "Loss @ time 168: 0.5194643139839172\n",
            "Loss @ time 169: 0.48100146651268005\n",
            "Loss @ time 170: 0.3832167088985443\n",
            "Loss @ time 171: 0.3236876428127289\n",
            "Loss @ time 172: 0.33810898661613464\n",
            "Loss @ time 173: 0.3769727051258087\n",
            "Loss @ time 174: 0.37850627303123474\n",
            "Loss @ time 175: 0.34420645236968994\n",
            "Loss @ time 176: 0.3209267556667328\n",
            "Loss @ time 177: 0.33734047412872314\n",
            "Loss @ time 178: 0.36811429262161255\n",
            "Loss @ time 179: 0.37141531705856323\n",
            "Loss @ time 180: 0.329549640417099\n",
            "Loss @ time 181: 0.2869223654270172\n",
            "Loss @ time 182: 0.2813197374343872\n",
            "Loss @ time 183: 0.3025343120098114\n",
            "Loss @ time 184: 0.32281383872032166\n",
            "Loss @ time 185: 0.3167608976364136\n",
            "Loss @ time 186: 0.2982857823371887\n",
            "Loss @ time 187: 0.28621330857276917\n",
            "Loss @ time 188: 0.29140493273735046\n",
            "Loss @ time 189: 0.3102850615978241\n",
            "Loss @ time 190: 0.31903910636901855\n",
            "Loss @ time 191: 0.3182360529899597\n",
            "Loss @ time 192: 0.3070666193962097\n",
            "Loss @ time 193: 0.30171269178390503\n",
            "Loss @ time 194: 0.30635878443717957\n",
            "Loss @ time 195: 0.31963905692100525\n",
            "Loss @ time 196: 0.335356742143631\n",
            "Loss @ time 197: 0.34471237659454346\n",
            "Loss @ time 198: 0.35679885745048523\n",
            "Loss @ time 199: 0.3803479075431824\n",
            "Loss @ time 200: 0.42256736755371094\n",
            "Loss @ time 201: 0.49684691429138184\n",
            "Loss @ time 202: 0.5604057908058167\n",
            "Loss @ time 203: 0.6143202185630798\n",
            "Loss @ time 204: 0.5881744623184204\n",
            "Loss @ time 205: 0.5283321142196655\n",
            "Loss @ time 206: 0.45830658078193665\n",
            "Loss @ time 207: 0.4083470106124878\n",
            "Loss @ time 208: 0.369566947221756\n",
            "Loss @ time 209: 0.3276158273220062\n",
            "Loss @ time 210: 0.3042934536933899\n",
            "Loss @ time 211: 0.32497578859329224\n",
            "Loss @ time 212: 0.38540637493133545\n",
            "Loss @ time 213: 0.45319706201553345\n",
            "Loss @ time 214: 0.4792826175689697\n",
            "Loss @ time 215: 0.45673564076423645\n",
            "Loss @ time 216: 0.40043404698371887\n",
            "Loss @ time 217: 0.34865161776542664\n",
            "Loss @ time 218: 0.3258054852485657\n",
            "Loss @ time 219: 0.31442326307296753\n",
            "Loss @ time 220: 0.30158331990242004\n",
            "Loss @ time 221: 0.29555946588516235\n",
            "Loss @ time 222: 0.30948320031166077\n",
            "Loss @ time 223: 0.3414347171783447\n",
            "Loss @ time 224: 0.3713696599006653\n",
            "Loss @ time 225: 0.39518073201179504\n",
            "Loss @ time 226: 0.38661354780197144\n",
            "Loss @ time 227: 0.36399781703948975\n",
            "Loss @ time 228: 0.33393919467926025\n",
            "Loss @ time 229: 0.3129146695137024\n",
            "Loss @ time 230: 0.3020463287830353\n",
            "Loss @ time 231: 0.2935120463371277\n",
            "Loss @ time 232: 0.2900004982948303\n",
            "Loss @ time 233: 0.2982718348503113\n",
            "Loss @ time 234: 0.31942829489707947\n",
            "Loss @ time 235: 0.348749577999115\n",
            "Loss @ time 236: 0.3739071786403656\n",
            "Loss @ time 237: 0.38925397396087646\n",
            "Loss @ time 238: 0.39554429054260254\n",
            "Loss @ time 239: 0.39468517899513245\n",
            "Loss @ time 240: 0.39157408475875854\n",
            "Loss @ time 241: 0.3728898763656616\n",
            "Loss @ time 242: 0.3478156626224518\n",
            "Loss @ time 243: 0.3126824200153351\n",
            "Loss @ time 244: 0.2864629626274109\n",
            "Loss @ time 245: 0.27465856075286865\n",
            "Loss @ time 246: 0.2762967050075531\n",
            "Loss @ time 247: 0.28471073508262634\n",
            "Loss @ time 248: 0.2969946265220642\n",
            "Loss @ time 249: 0.3135892152786255\n",
            "Loss @ time 250: 0.32655590772628784\n",
            "Loss @ time 251: 0.33881092071533203\n",
            "Loss @ time 252: 0.3500300347805023\n",
            "Loss @ time 253: 0.3624719977378845\n",
            "Loss @ time 254: 0.374673455953598\n",
            "Loss @ time 255: 0.3859642446041107\n",
            "Loss @ time 256: 0.39315736293792725\n",
            "Loss @ time 257: 0.4020234942436218\n",
            "Loss @ time 258: 0.40100204944610596\n",
            "Loss @ time 259: 0.3947093188762665\n",
            "Loss @ time 260: 0.374948114156723\n",
            "Loss @ time 261: 0.3516761362552643\n",
            "Loss @ time 262: 0.3269980549812317\n",
            "Loss @ time 263: 0.30783611536026\n",
            "Loss @ time 264: 0.29471269249916077\n",
            "Loss @ time 265: 0.2861214280128479\n",
            "Loss @ time 266: 0.2796577215194702\n",
            "Loss @ time 267: 0.2761024236679077\n",
            "Loss @ time 268: 0.27404308319091797\n",
            "Loss @ time 269: 0.2734629511833191\n",
            "Loss @ time 270: 0.27294883131980896\n",
            "Loss @ time 271: 0.2730897068977356\n",
            "Loss @ time 272: 0.27377182245254517\n",
            "Loss @ time 273: 0.2749858498573303\n",
            "Loss @ time 274: 0.27613526582717896\n",
            "Loss @ time 275: 0.27885618805885315\n",
            "Loss @ time 276: 0.28467249870300293\n",
            "Loss @ time 277: 0.29563969373703003\n",
            "Loss @ time 278: 0.3156055212020874\n",
            "Loss @ time 279: 0.35309678316116333\n",
            "Loss @ time 280: 0.41539669036865234\n",
            "Loss @ time 281: 0.5352677702903748\n",
            "Loss @ time 282: 0.7106680274009705\n",
            "Loss @ time 283: 0.9628260135650635\n",
            "Loss @ time 284: 1.1062027215957642\n",
            "Loss @ time 285: 1.1366605758666992\n",
            "Loss @ time 286: 0.9354890584945679\n",
            "Loss @ time 287: 0.6025499105453491\n",
            "Loss @ time 288: 0.3422950804233551\n",
            "Loss @ time 289: 0.32048091292381287\n",
            "Loss @ time 290: 0.4809247553348541\n",
            "Loss @ time 291: 0.6473074555397034\n",
            "Loss @ time 292: 0.7053318023681641\n",
            "Loss @ time 293: 0.5809207558631897\n",
            "Loss @ time 294: 0.38931727409362793\n",
            "Loss @ time 295: 0.2983987033367157\n",
            "Loss @ time 296: 0.3521726131439209\n",
            "Loss @ time 297: 0.4496949017047882\n",
            "Loss @ time 298: 0.4655954837799072\n",
            "Loss @ time 299: 0.3792903423309326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md4TtaoW1CyG"
      },
      "source": [
        "## Evaluating our Model\n",
        "\n",
        "Now that we've trained our network, let's look at how well we did. To do this, we will feed in a range of theta values and see how well our networks predictions line up with the ground truth dynamics of our system. We plot the original data in blue, and our neural networks fit in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kep9pA0Vsh8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "03385b07-f7db-4bc6-f465-b296613a2550"
      },
      "source": [
        "if optimizer is not None and L is not None:\n",
        "  theta_eval = torch.unsqueeze(torch.linspace(-3*np.pi, 3*np.pi, 10000), dim=1)\n",
        "  theta_ground = np.linspace(-2*np.pi, 2*np.pi, 100)\n",
        "  theta_dot_ground = u - m*g*l*np.sin(theta_ground)\n",
        "\n",
        "  # NOTE: some layers like batchnorm or dropout layers need to be turned\n",
        "  # off during evaluation and turned on during training. Calling model.eval()\n",
        "  # will do this for you. We do not use these types of layers here, but we\n",
        "  # call eval() here to demonstrate for potential future use. To go back to \"training\"\n",
        "  # mode, call model.train()\n",
        "  NN.eval()\n",
        "\n",
        "  # evaluate our network\n",
        "  theta_dot_eval = NN(theta_eval)\n",
        "\n",
        "  # plot our fit\n",
        "  plt.scatter(theta_ground, theta_dot_ground, s=6)\n",
        "  plt.plot(theta_eval, theta_dot_eval.data.numpy(), color='red', lw=1) # this is how to convert a pytorch tensor to a numpy array\n",
        "  plt.plot(np.linspace(-4*np.pi, 4*np.pi, 3), np.zeros(3), color='black', lw=1)\n",
        "  plt.xlim(-3*np.pi, 3*np.pi)\n",
        "  plt.xlabel('theta')\n",
        "  plt.ylabel('theta_dot')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44cLk3K6x4ZY"
      },
      "source": [
        "## Written questions\n",
        "\n",
        "Answer these written exercises in a pdf (preferably latex) and upload it gradescope.\n",
        "\n",
        "1) Within the range $\\theta \\in [-5.0, 5.0]$, which values of $(\\theta, \\dot \\theta)$ represent fixed points? Which fixed points are stable and which are unstable?\n",
        "\n",
        "2) What is one reason why our model might not fit the ground truth data exactly?\n",
        "\n",
        "3) Does our model give us a reasonable approximation of the dynamics for all $(\\theta, \\dot \\theta)$ pairs? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-T-a5gAh3OH"
      },
      "source": [
        "## How Will this Notebook Be Graded?\n",
        "If you are enrolled in the class, this notebook will be graded using [Gradescope](https://www.gradescope.com).\n",
        "We will send you the details of how to access the course page in Gradescope by email.\n",
        "\n",
        "We will replicate your work by running your notebook and checking that the final loss value is within a reasonable range.\n",
        "\n",
        "You will get full score if the following test succeeds:\n",
        "- Training runs and the final loss value is within a reasonable range of the value we are generating.\n",
        "\n",
        "This should hold if you define your network appropriately, and choose the right optimizer/loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgUjMUG_h3OI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f9941a-ce48-4245-91f2-8eaccfbed378"
      },
      "source": [
        "from underactuated.exercises.pend.learning_dynamics.test_learning_dynamics import TestLearningDynamics\n",
        "from underactuated.exercises.grader import Grader\n",
        "Grader.grade_output([TestLearningDynamics], [locals()], 'results.json')\n",
        "Grader.print_test_results('results.json')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total score is 5/5.\n",
            "\n",
            "Score for Test dynamics model training is 5/5.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}